{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37bffb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdce44b3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataSets/THA2train.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m validate_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_excel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataSets/THA2validate.xlsx\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Read data\n",
    "train_data = pd.read_excel('DataSets/THA2train.xlsx')\n",
    "validate_data = pd.read_excel('DataSets/THA2validate.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6707efeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data separation features and labels\n",
    "X_train = train_data.iloc[:, :-1].values\n",
    "y_train = pd.get_dummies(train_data.iloc[:, -1]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad23f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data separation features and labels\n",
    "X_validate = validate_data.iloc[:, :-1].values\n",
    "y_validate = pd.get_dummies(validate_data.iloc[:, -1]).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50b68b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this function is for sigmoid by handeling overflow\n",
    "def sigmoid(z):\n",
    "    return np.where(z >= 0, \n",
    "                    1 / (1 + np.exp(-z)), \n",
    "                    np.exp(z) / (1 + np.exp(z)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15581f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#derivative of sigmoid\n",
    "def sigmoid_derivative(a):\n",
    "    return a * (1 - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47df043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax function\n",
    "def softmax(z):\n",
    "    e_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return e_z / np.sum(e_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be131f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "#the MLP class\n",
    "class MLP:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2 / (input_size + hidden_size))\n",
    "        self.b1 = np.zeros((1, hidden_size))\n",
    "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2 / (hidden_size + output_size))\n",
    "        self.b2 = np.zeros((1, output_size))\n",
    "\n",
    "        \n",
    "\n",
    "    def forward(self, X):\n",
    "        self.z1 = np.dot(X, self.W1) + self.b1\n",
    "        self.a1 = sigmoid(self.z1)\n",
    "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
    "        self.a2 = softmax(self.z2)\n",
    "        return self.a2\n",
    "\n",
    "    def backward(self, X, y):\n",
    "        output_error = self.a2 - y\n",
    "        dW2 = np.dot(self.a1.T, output_error)\n",
    "        db2 = np.sum(output_error, axis=0, keepdims=True)\n",
    "        \n",
    "        hidden_error = np.dot(output_error, self.W2.T) * sigmoid_derivative(self.a1)\n",
    "        dW1 = np.dot(X.T, hidden_error)\n",
    "        db1 = np.sum(hidden_error, axis=0, keepdims=True)\n",
    "        \n",
    "        return dW1, db1, dW2, db2\n",
    "\n",
    "    def update_weights(self, dW1, db1, dW2, db2, learning_rate):\n",
    "        self.W1 -= learning_rate * dW1\n",
    "        self.b1 -= learning_rate * db1\n",
    "        self.W2 -= learning_rate * dW2\n",
    "        self.b2 -= learning_rate * db2\n",
    "\n",
    "    def compute_loss(self, y_true, y_pred):\n",
    "        m = y_true.shape[0]\n",
    "        loss = -np.sum(y_true * np.log(y_pred + 1e-8)) / m  \n",
    "        return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c5c4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the data\n",
    "X_train_norm = (X_train - np.mean(X_train, axis=0)) / np.std(X_train, axis=0)\n",
    "X_validate_norm = (X_validate - np.mean(X_validate, axis=0)) / np.std(X_validate, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf388110",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 2000\n",
    "learning_rate = 0.01\n",
    "batch_size = 32\n",
    "\n",
    "# Lists to store loss for plotting\n",
    "training_losses = []\n",
    "validation_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43426082",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MLP' object has no attribute '_validate_params'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[169], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m param_dist \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_size\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m50\u001b[39m, \u001b[38;5;241m100\u001b[39m, \u001b[38;5;241m150\u001b[39m],\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.001\u001b[39m, \u001b[38;5;241m0.01\u001b[39m, \u001b[38;5;241m0.1\u001b[39m],\n\u001b[0;32m      4\u001b[0m }\n\u001b[0;32m      6\u001b[0m mlp \u001b[38;5;241m=\u001b[39m MLP(input_size\u001b[38;5;241m=\u001b[39mX_train_norm\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], hidden_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, output_size\u001b[38;5;241m=\u001b[39my_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m----> 7\u001b[0m random_search \u001b[38;5;241m=\u001b[39m RandomizedSearchCV\u001b[38;5;241m.\u001b[39mfit(mlp, param_distributions\u001b[38;5;241m=\u001b[39mparam_dist, n_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, cv\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, scoring\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Fit RandomizedSearchCV to your training data\u001b[39;00m\n\u001b[0;32m     10\u001b[0m random_search\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n",
      "File \u001b[1;32mc:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1144\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1140\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1141\u001b[0m )\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1144\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1146\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1147\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1148\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1149\u001b[0m     )\n\u001b[0;32m   1150\u001b[0m ):\n\u001b[0;32m   1151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'MLP' object has no attribute '_validate_params'"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "    'hidden_size': [50, 100, 150],\n",
    "    'learning_rate': [0.001, 0.01, 0.1],\n",
    "}\n",
    "\n",
    "mlp = MLP(input_size=X_train_norm.shape[1], hidden_size=10, output_size=y_train.shape[1])\n",
    "random_search = RandomizedSearchCV.fit(mlp, param_distributions=param_dist, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit RandomizedSearchCV to your training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_mlp = random_search.best_estimator_\n",
    "random_search.fit(X_train_norm, y_train)\n",
    "\n",
    "# Get the best and worst models based on validation accuracy\n",
    "# Get the best and worst models based on validation accuracy\n",
    "best_index = np.argmax(random_search.cv_results_['mean_test_score'])\n",
    "worst_index = np.argmin(random_search.cv_results_['mean_test_score'])\n",
    "\n",
    "best_params = random_search.cv_results_['params'][best_index]\n",
    "worst_params = random_search.cv_results_['params'][worst_index]\n",
    "\n",
    "best_model = MLP(input_size=X_train_norm.shape[1], output_size=y_train.shape[1], **best_params)\n",
    "worst_model = MLP(input_size=X_train_norm.shape[1], output_size=y_train.shape[1], **worst_params)\n",
    "\n",
    "# Train the best and worst models again to obtain training and validation losses\n",
    "train_losses_best, validation_losses_best = [], []\n",
    "train_losses_worst, validation_losses_worst = [], []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    indices_best = np.arange(X_train_norm.shape[0])\n",
    "    np.random.shuffle(indices_best)\n",
    "    X_train_norm_shuffled_best = X_train_norm[indices_best]\n",
    "    y_train_shuffled_best = y_train[indices_best]\n",
    "\n",
    "    indices_worst = np.arange(X_train_norm.shape[0])\n",
    "    np.random.shuffle(indices_worst)\n",
    "    X_train_norm_shuffled_worst = X_train_norm[indices_worst]\n",
    "    y_train_shuffled_worst = y_train[indices_worst]\n",
    "\n",
    "    for start_idx in range(0, X_train_norm.shape[0], batch_size):\n",
    "        end_idx = min(start_idx + batch_size, X_train_norm.shape[0])\n",
    "\n",
    "        # Training for the best model\n",
    "        X_batch_best = X_train_norm_shuffled_best[start_idx:end_idx]\n",
    "        y_batch_best = y_train_shuffled_best[start_idx:end_idx]\n",
    "        predictions_best = best_model.forward(X_batch_best)\n",
    "        dW1_best, db1_best, dW2_best, db2_best = best_model.backward(X_batch_best, y_batch_best)\n",
    "        best_model.update_weights(dW1_best, db1_best, dW2_best, db2_best, learning_rate)\n",
    "\n",
    "        # Training for the worst model\n",
    "        X_batch_worst = X_train_norm_shuffled_worst[start_idx:end_idx]\n",
    "        y_batch_worst = y_train_shuffled_worst[start_idx:end_idx]\n",
    "        predictions_worst = worst_model.forward(X_batch_worst)\n",
    "        dW1_worst, db1_worst, dW2_worst, db2_worst = worst_model.backward(X_batch_worst, y_batch_worst)\n",
    "        worst_model.update_weights(dW1_worst, db1_worst, dW2_worst, db2_worst, learning_rate)\n",
    "\n",
    "    # Compute losses for the best model\n",
    "    train_loss_best = best_model.compute_loss(y_train, best_model.forward(X_train_norm))\n",
    "    validation_loss_best = best_model.compute_loss(y_validate, best_model.forward(X_validate_norm))\n",
    "    train_losses_best.append(train_loss_best)\n",
    "    validation_losses_best.append(validation_loss_best)\n",
    "\n",
    "    # Compute losses for the worst model\n",
    "    train_loss_worst = worst_model.compute_loss(y_train, worst_model.forward(X_train_norm))\n",
    "    validation_loss_worst = worst_model.compute_loss(y_validate, worst_model.forward(X_validate_norm))\n",
    "    train_losses_worst.append(train_loss_worst)\n",
    "    validation_losses_worst.append(validation_loss_worst)\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch}, Best Model: Training Loss: {train_loss_best:.4f}, Validation Loss: {validation_loss_best:.4f}, \"\n",
    "            f\"Worst Model: Training Loss: {train_loss_worst:.4f}, Validation Loss: {validation_loss_worst:.4f}\"\n",
    "        )\n",
    "mlp = MLPClassifier(max_iter=100)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(mlp, param_distributions=param_dist, n_iter=10, cv=3, scoring='accuracy', random_state=42, n_jobs=-1)\n",
    "\n",
    "# Fit RandomizedSearchCV to your training data\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model\n",
    "best_mlp = random_search.best_estimator_\n",
    "\n",
    "# Predict on validation set and calculate accuracy\n",
    "y_pred = best_mlp.predict(X_val)\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Validation Accuracy of Best Model: {accuracy:.4f}\")\n",
    "# Plot training and validation losses for the best and worst models\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses_best, label='Best Model - Training')\n",
    "plt.plot(validation_losses_best, label='Best Model - Validation')\n",
    "plt.title('Best Model - Training and Validation Losses')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_losses_worst, label='Worst Model - Training')\n",
    "plt.plot(validation_losses_worst, label='Worst Model - Validation')\n",
    "plt.title('Worst Model - Training and Validation Losses')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33773978",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
